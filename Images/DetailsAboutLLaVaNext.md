[LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild](https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/)
This blog post discusses the integration of stronger large language models (LLMs) into LLaVA-NeXT, enhancing its multimodal capabilities. [link]( https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/)

[LLaVA-NeXT: Improved Reasoning, OCR, and World Knowledge](https://llava-vl.github.io/blog/2024-01-30-llava-next/)
This article outlines improvements in reasoning, optical character recognition (OCR), and world knowledge in LLaVA-NeXT models.

[LLaVA-NeXT: A Strong Zero-shot Video Understanding Model](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/)
This publication explores LLaVA-NeXT's performance in video understanding tasks without explicit video training.

[LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models](https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/)
This blog post addresses LLaVA-NeXT's capabilities in handling multi-image, video, and 3D data.

[LLaVA-NeXT - GitHub Repository](https://github.com/LLaVA-VL/LLaVA-NeXT)
The official GitHub repository provides access to the codebase, models, and additional documentation for LLaVA-NeXT.
